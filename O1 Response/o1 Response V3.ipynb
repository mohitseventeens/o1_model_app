{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4594e237",
   "metadata": {
    "height": 2308
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from helper import get_openai_api_key\n",
    "from openai import OpenAI\n",
    "import markdown\n",
    "\n",
    "class O1ResponseGenerator:\n",
    "    def __init__(self, o1_model=\"o1-mini\"):\n",
    "        \"\"\"\n",
    "        Initialize the O1ResponseGenerator with a model name.\n",
    "        \"\"\"\n",
    "        self.o1_model = o1_model\n",
    "        openai_api_key = get_openai_api_key()\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    def _calculate_cost(self, prompt_tokens, cached_input_tokens, output_tokens):\n",
    "        \"\"\"\n",
    "        Internal method to calculate the total cost based on tokens used.\n",
    "        \"\"\"\n",
    "        model_prices = {\n",
    "            \"o1\": {\n",
    "                \"input_token_price\": 15.00 / 1_000_000,\n",
    "                \"cached_input_token_price\": 7.50 / 1_000_000,\n",
    "                \"output_token_price\": 60.00 / 1_000_000,\n",
    "            },\n",
    "            \"o1-mini\": {\n",
    "                \"input_token_price\": 3.00 / 1_000_000,\n",
    "                \"cached_input_token_price\": 1.50 / 1_000_000,\n",
    "                \"output_token_price\": 12.00 / 1_000_000,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        if self.o1_model not in model_prices:\n",
    "            self.o1_model = \"o1\"\n",
    "\n",
    "        prices = model_prices[self.o1_model]\n",
    "        input_token_cost = (prompt_tokens - cached_input_tokens) * prices[\"input_token_price\"]\n",
    "        cached_input_token_cost = cached_input_tokens * prices[\"cached_input_token_price\"]\n",
    "        output_token_cost = output_tokens * prices[\"output_token_price\"]\n",
    "\n",
    "        return input_token_cost + cached_input_token_cost + output_token_cost\n",
    "\n",
    "    def _save_response_html(self, file_name, response_time, total_cost, response_content):\n",
    "        \"\"\"\n",
    "        Internal method to generate and save HTML from the response content.\n",
    "        \"\"\"\n",
    "        response_html = markdown.markdown(response_content)\n",
    "        html_content = f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <title>O1 Response</title>\n",
    "            <script type=\"text/javascript\" async \n",
    "                src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "            </script>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div style=\"background-color: #f0fff8; padding: 10px; border-radius: 5px; border: 1px solid #d3d3d3;\">\n",
    "                <h2>{self.o1_model} Model Response</h2>\n",
    "                <p>Response time: {response_time:.2f} seconds</p>\n",
    "                <p>Total Cost: {total_cost:.2f} $</p>\n",
    "            </div>\n",
    "\n",
    "            <div style=\"padding: 10px;\">\n",
    "                {response_html}\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html_content)\n",
    "\n",
    "        print(f\"Response saved to {file_name}\")\n",
    "\n",
    "    def generate_response(self, prompt, system_prompt, file_name=\"o1_response_test.html\"):\n",
    "        \"\"\"\n",
    "        Public method to generate a response using the O1 model.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.o1_model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"<System Prompt>{system_prompt}</System Prompt>\\\\n<USER Prompt>{prompt}</USER Prompt>\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        response_time = time.time() - start_time\n",
    "        usage = response.usage\n",
    "\n",
    "        total_cost = self._calculate_cost(\n",
    "            usage.prompt_tokens,\n",
    "            usage.prompt_tokens_details.cached_tokens,\n",
    "            usage.completion_tokens\n",
    "        )\n",
    "\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "        # Save to HTML\n",
    "        self._save_response_html(file_name, response_time, total_cost, response_content)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ebe792",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "- You are an expert Python programmer and refactoring specialist.\n",
    "- Before refactoring you need to Identify flaws in logic, inefficiencies, potential bugs and correct those.\n",
    "- Your task is to refactor the provided Python code step by step, convert the code into object-oriented approach.\n",
    "- Provide the following for each step:\n",
    "  - A brief explanation of what is being refactored and why the change is necessary.\n",
    "  - The original code snippet and the refactored version, presented side by side using Python code blocks.\n",
    "  - Highlight the improvements in the refactored version, such as enhanced readability, better performance, or bug fixes.\n",
    "- If additional functionality or changes in design are needed (e.g., converting the code to an object-oriented approach or refactoring for async programming), explain the rationale before implementing.\n",
    "- Structure the output using HTML elements. So it should include all appropriate HTML elements.\n",
    "  - blocks for Python code and ensure proper syntax highlighting.\n",
    "- Ensure the tone is professional, constructive, and educational.\n",
    "- Keep the layout clean and intuitive, ensuring the user can easily follow the suggestions and apply them effectively.\n",
    "\"\"\"\n",
    "\n",
    "prompt = '''\n",
    "import time\n",
    "from helper import get_openai_api_key\n",
    "from openai import OpenAI\n",
    "import markdown\n",
    "\n",
    "class O1ResponseGenerator:\n",
    "    def __init__(self, o1_model=\"o1\"):\n",
    "        \"\"\"\n",
    "        Initialize the O1ResponseGenerator with a model name.\n",
    "        \"\"\"\n",
    "        self.o1_model = o1_model\n",
    "        openai_api_key = get_openai_api_key()\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    def _calculate_cost(self, prompt_tokens, cached_input_tokens, output_tokens):\n",
    "        \"\"\"\n",
    "        Internal method to calculate the total cost based on tokens used.\n",
    "        \"\"\"\n",
    "        model_prices = {\n",
    "            \"o1\": {\n",
    "                \"input_token_price\": 15.00 / 1_000_000,\n",
    "                \"cached_input_token_price\": 7.50 / 1_000_000,\n",
    "                \"output_token_price\": 60.00 / 1_000_000,\n",
    "            },\n",
    "            \"o1-mini\": {\n",
    "                \"input_token_price\": 3.00 / 1_000_000,\n",
    "                \"cached_input_token_price\": 1.50 / 1_000_000,\n",
    "                \"output_token_price\": 12.00 / 1_000_000,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        if self.o1_model not in model_prices:\n",
    "            self.o1_model = \"o1\"\n",
    "\n",
    "        prices = model_prices[self.o1_model]\n",
    "        input_token_cost = (prompt_tokens - cached_input_tokens) * prices[\"input_token_price\"]\n",
    "        cached_input_token_cost = cached_input_tokens * prices[\"cached_input_token_price\"]\n",
    "        output_token_cost = output_tokens * prices[\"output_token_price\"]\n",
    "\n",
    "        return input_token_cost + cached_input_token_cost + output_token_cost\n",
    "\n",
    "    def _save_response_html(self, file_name, response_time, total_cost, response_content):\n",
    "        \"\"\"\n",
    "        Internal method to generate and save HTML from the response content.\n",
    "        \"\"\"\n",
    "        response_html = markdown.markdown(response_content)\n",
    "        html_content = f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <title>O1 Response</title>\n",
    "            <script type=\"text/javascript\" async \n",
    "                src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "            </script>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div style=\"background-color: #f0fff8; padding: 10px; border-radius: 5px; border: 1px solid #d3d3d3;\">\n",
    "                <h2>{self.o1_model} Model Response</h2>\n",
    "                <p>Response time: {response_time:.2f} seconds</p>\n",
    "                <p>Total Cost: {total_cost:.2f} $</p>\n",
    "            </div>\n",
    "\n",
    "            <div style=\"padding: 10px;\">\n",
    "                {response_html}\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html_content)\n",
    "\n",
    "        print(f\"Response saved to {file_name}\")\n",
    "\n",
    "    def generate_response(self, prompt, system_prompt, file_name=\"o1_response_test.html\", display_output=False):\n",
    "        \"\"\"\n",
    "        Public method to generate a response using the O1 model.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.o1_model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"<System Prompt>{system_prompt}</System Prompt>\\\\n<USER Prompt>{prompt}</USER Prompt>\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        response_time = time.time() - start_time\n",
    "        usage = response.usage\n",
    "\n",
    "        total_cost = self._calculate_cost(\n",
    "            usage.prompt_tokens,\n",
    "            usage.prompt_tokens_details.cached_tokens,\n",
    "            usage.completion_tokens\n",
    "        )\n",
    "\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "        if display_output:\n",
    "            print(f\"Model: {self.o1_model}\")\n",
    "            print(f\"Response time: {response_time:.2f} seconds\")\n",
    "            print(f\"Total Cost: {total_cost:.2f} $\\\\n\")\n",
    "            print(response_content)\n",
    "\n",
    "        # Save to HTML\n",
    "        self._save_response_html(file_name, response_time, total_cost, response_content)\n",
    "\n",
    "        return response\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade83f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: o1-mini\n",
      "Response time: 47.26 seconds\n",
      "Total Cost: 0.07 $\\n\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <title>Step-by-Step Refactoring of O1ResponseGenerator</title>\n",
      "    <style>\n",
      "        body { font-family: Arial, sans-serif; margin: 20px; }\n",
      "        h2 { color: #2E8B57; }\n",
      "        .step { margin-bottom: 40px; }\n",
      "        .explanation { margin-bottom: 10px; }\n",
      "        .code-container { display: flex; gap: 20px; }\n",
      "        pre { background-color: #f5f5f5; padding: 10px; border-radius: 5px; overflow-x: auto; width: 48%; }\n",
      "        .improvements { margin-top: 10px; color: #555555; }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>Refactoring of `O1ResponseGenerator` Class</h1>\n",
      "\n",
      "    <div class=\"step\">\n",
      "        <h2>Step 1: Optimize Import Statements</h2>\n",
      "        <div class=\"explanation\">\n",
      "            <p>Optimizing import statements enhances code readability and ensures that only necessary modules are loaded. Additionally, organizing imports into standard library, third-party, and local modules improves maintainability.</p>\n",
      "        </div>\n",
      "        <div class=\"code-container\">\n",
      "            <div>\n",
      "                <h3>Original Code</h3>\n",
      "                <pre><code class=\"python\">import time\n",
      "from helper import get_openai_api_key\n",
      "from openai import OpenAI\n",
      "import markdown</code></pre>\n",
      "            </div>\n",
      "            <div>\n",
      "                <h3>Refactored Code</h3>\n",
      "                <pre><code class=\"python\">import time\n",
      "import markdown\n",
      "\n",
      "from openai import OpenAI\n",
      "\n",
      "from helper import get_openai_api_key</code></pre>\n",
      "            </div>\n",
      "        </div>\n",
      "        <div class=\"improvements\">\n",
      "            <p><strong>Improvements:</strong> Organized imports into standard libraries (`time`), third-party libraries (`markdown`, `openai`), and local modules (`helper`). This structure enhances readability and maintainability.</p>\n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"step\">\n",
      "        <h2>Step 2: Enhance Error Handling During Initialization</h2>\n",
      "        <div class=\"explanation\">\n",
      "            <p>Adding error handling during the initialization of the OpenAI client ensures that issues like missing API keys are caught early, preventing unexpected crashes during runtime.</p>\n",
      "        </div>\n",
      "        <div class=\"code-container\">\n",
      "            <div>\n",
      "                <h3>Original Code</h3>\n",
      "                <pre><code class=\"python\">def __init__(self, o1_model=\"o1\"):\n",
      "    \"\"\"\n",
      "    Initialize the O1ResponseGenerator with a model name.\n",
      "    \"\"\"\n",
      "    self.o1_model = o1_model\n",
      "    openai_api_key = get_openai_api_key()\n",
      "    self.client = OpenAI(api_key=openai_api_key)</code></pre>\n",
      "            </div>\n",
      "            <div>\n",
      "                <h3>Refactored Code</h3>\n",
      "                <pre><code class=\"python\">def __init__(self, o1_model: str = \"o1\"):\n",
      "    \"\"\"\n",
      "    Initialize the O1ResponseGenerator with a model name.\n",
      "    \"\"\"\n",
      "    self.o1_model = o1_model\n",
      "    try:\n",
      "        openai_api_key = get_openai_api_key()\n",
      "        if not openai_api_key:\n",
      "            raise ValueError(\"OpenAI API key is missing.\")\n",
      "        self.client = OpenAI(api_key=openai_api_key)\n",
      "    except Exception as e:\n",
      "        raise RuntimeError(f\"Failed to initialize OpenAI client: {e}\") from e</code></pre>\n",
      "            </div>\n",
      "        </div>\n",
      "        <div class=\"improvements\">\n",
      "            <p><strong>Improvements:</strong> Added a type hint for `o1_model` and implemented a try-except block to catch and handle errors related to missing or invalid API keys. This ensures robust initialization and clearer error messages.</p>\n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"step\">\n",
      "        <h2>Step 3: Refactor Token Pricing Configuration</h2>\n",
      "        <div class=\"explanation\">\n",
      "            <p>Encapsulating token pricing within a dedicated data structure enhances scalability and maintainability. Using a dataclass provides a clear and immutable structure for model pricing.</p>\n",
      "        </div>\n",
      "        <div class=\"code-container\">\n",
      "            <div>\n",
      "                <h3>Original Code</h3>\n",
      "                <pre><code class=\"python\">model_prices = {\n",
      "    \"o1\": {\n",
      "        \"input_token_price\": 15.00 / 1_000_000,\n",
      "        \"cached_input_token_price\": 7.50 / 1_000_000,\n",
      "        \"output_token_price\": 60.00 / 1_000_000,\n",
      "    },\n",
      "    \"o1-mini\": {\n",
      "        \"input_token_price\": 3.00 / 1_000_000,\n",
      "        \"cached_input_token_price\": 1.50 / 1_000_000,\n",
      "        \"output_token_price\": 12.00 / 1_000_000,\n",
      "    },\n",
      "}</code></pre>\n",
      "            </div>\n",
      "            <div>\n",
      "                <h3>Refactored Code</h3>\n",
      "                <pre><code class=\"python\">from dataclasses import dataclass\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class ModelPrices:\n",
      "    input_token_price: float\n",
      "    cached_input_token_price: float\n",
      "    output_token_price: float\n",
      "\n",
      "MODEL_PRICES = {\n",
      "    \"o1\": ModelPrices(\n",
      "        input_token_price=15.00 / 1_000_000,\n",
      "        cached_input_token_price=7.50 / 1_000_000,\n",
      "        output_token_price=60.00 / 1_000_000,\n",
      "    ),\n",
      "    \"o1-mini\": ModelPrices(\n",
      "        input_token_price=3.00 / 1_000_000,\n",
      "        cached_input_token_price=1.50 / 1_000_000,\n",
      "        output_token_price=12.00 / 1_000_000,\n",
      "    ),\n",
      "}</code></pre>\n",
      "            </div>\n",
      "        </div>\n",
      "        <div class=\"improvements\">\n",
      "            <p><strong>Improvements:</strong> Introduced a `ModelPrices` dataclass to represent token pricing for each model. This approach provides immutability and clarity, making the pricing structure easier to manage and extend.</p>\n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"step\">\n",
      "        <h2>Step 4: Improve the `_calculate_cost` Method</h2>\n",
      "        <div class=\"explanation\">\n",
      "            <p>Refactoring `_calculate_cost` to utilize the `ModelPrices` dataclass enhances code readability and ensures consistent access to pricing attributes. Additionally, handling unknown models gracefully improves the method's robustness.</p>\n",
      "        </div>\n",
      "        <div class=\"code-container\">\n",
      "            <div>\n",
      "                <h3>Original Code</h3>\n",
      "                <pre><code class=\"python\">def _calculate_cost(self, prompt_tokens, cached_input_tokens, output_tokens):\n",
      "    \"\"\"\n",
      "    Internal method to calculate the total cost based on tokens used.\n",
      "    \"\"\"\n",
      "    model_prices = { ... }  # As defined previously\n",
      "\n",
      "    if self.o1_model not in model_prices:\n",
      "        self.o1_model = \"o1\"\n",
      "\n",
      "    prices = model_prices[self.o1_model]\n",
      "    input_token_cost = (prompt_tokens - cached_input_tokens) * prices[\"input_token_price\"]\n",
      "    cached_input_token_cost = cached_input_tokens * prices[\"cached_input_token_price\"]\n",
      "    output_token_cost = output_tokens * prices[\"output_token_price\"]\n",
      "\n",
      "    return input_token_cost + cached_input_token_cost + output_token_cost</code></pre>\n",
      "            </div>\n",
      "            <div>\n",
      "                <h3>Refactored Code</h3>\n",
      "                <pre><code class=\"python\">def _calculate_cost(self, prompt_tokens: int, cached_input_tokens: int, output_tokens: int) -> float:\n",
      "    \"\"\"\n",
      "    Internal method to calculate the total cost based on tokens used.\n",
      "    \"\"\"\n",
      "    prices = MODEL_PRICES.get(self.o1_model, MODEL_PRICES[\"o1\"])\n",
      "\n",
      "    input_token_cost = (prompt_tokens - cached_input_tokens) * prices.input_token_price\n",
      "    cached_input_token_cost = cached_input_tokens * prices.cached_input_token_price\n",
      "    output_token_cost = output_tokens * prices.output_token_price\n",
      "\n",
      "    total_cost = input_token_cost + cached_input_token_cost + output_token_cost\n",
      "    return total_cost</code></pre>\n",
      "            </div>\n",
      "        </div>\n",
      "        <div class=\"improvements\">\n",
      "            <p><strong>Improvements:</strong> Leveraged the `MODEL_PRICES` dataclass for cleaner attribute access. Replaced the manual check with `get` for safer retrieval of model pricing, defaulting to `\"o1\"` if the model is unknown. Added type hints for better code clarity and type checking.</p>\n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"step\">\n",
      "        <h2>Step 5: Enhance the `_save_response_html` Method</h2>\n",
      "        <div class=\"explanation\">\n",
      "            <p>Refactoring the HTML generation to use Python's `str.format` method improves readability. Additionally, separating the HTML template from the method logic facilitates easier modifications and maintenance. Adding type hints enhances code clarity.</p>\n",
      "        </div>\n",
      "        <div class=\"code-container\">\n",
      "            <div>\n",
      "                <h3>Original Code</h3>\n",
      "                <pre><code class=\"python\">def _save_response_html(self, file_name, response_time, total_cost, response_content):\n",
      "    \"\"\"\n",
      "    Internal method to generate and save HTML from the response content.\n",
      "    \"\"\"\n",
      "    response_html = markdown.markdown(response_content)\n",
      "    html_content = f\"\"\"\n",
      "    <html>\n",
      "    <head>\n",
      "        <meta charset=\"UTF-8\">\n",
      "        <title>O1 Response</title>\n",
      "        <script type=\"text/javascript\" async \n",
      "            src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "        </script>\n",
      "    </head>\n",
      "    <body>\n",
      "        <div style=\"background-color: #f0fff8; padding: 10px; border-radius: 5px; border: 1px solid #d3d3d3;\">\n",
      "            <h2>{self.o1_model} Model Response</h2>\n",
      "            <p>Response time: {response_time:.2f} seconds</p>\n",
      "            <p>Total Cost: {total_cost:.2f} $</p>\n",
      "        </div>\n",
      "\n",
      "        <div style=\"padding: 10px;\">\n",
      "            {response_html}\n",
      "        </div>\n",
      "    </body>\n",
      "    </html>\n",
      "    \"\"\"\n",
      "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
      "        file.write(html_content)\n",
      "\n",
      "    print(f\"Response saved to {file_name}\")</code></pre>\n",
      "            </div>\n",
      "            <div>\n",
      "                <h3>Refactored Code</h3>\n",
      "                <pre><code class=\"python\">def _save_response_html(\n",
      "    self, \n",
      "    file_name: str, \n",
      "    response_time: float, \n",
      "    total_cost: float, \n",
      "    response_content: str\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Internal method to generate and save HTML from the response content.\n",
      "    \"\"\"\n",
      "    response_html = markdown.markdown(response_content)\n",
      "    html_template = \"\"\"\n",
      "    <html>\n",
      "    <head>\n",
      "        <meta charset=\"UTF-8\">\n",
      "        <title>O1 Response</title>\n",
      "        <script type=\"text/javascript\" async \n",
      "            src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "        </script>\n",
      "    </head>\n",
      "    <body>\n",
      "        <div style=\"background-color: #f0fff8; padding: 10px; border-radius: 5px; border: 1px solid #d3d3d3;\">\n",
      "            <h2>{model} Model Response</h2>\n",
      "            <p>Response time: {time:.2f} seconds</p>\n",
      "            <p>Total Cost: {cost:.2f} $</p>\n",
      "        </div>\n",
      "\n",
      "        <div style=\"padding: 10px;\">\n",
      "            {content}\n",
      "        </div>\n",
      "    </body>\n",
      "    </html>\n",
      "    \"\"\"\n",
      "    html_content = html_template.format(\n",
      "        model=self.o1_model,\n",
      "        time=response_time,\n",
      "        cost=total_cost,\n",
      "        content=response_html\n",
      "    )\n",
      "    try:\n",
      "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
      "            file.write(html_content)\n",
      "        print(f\"Response saved to {file_name}\")\n",
      "    except IOError as e:\n",
      "        print(f\"Failed to save response to {file_name}: {e}\")</code></pre>\n",
      "            </div>\n",
      "        </div>\n",
      "        <div class=\"improvements\">\n",
      "            <p><strong>Improvements:</strong> Separated the HTML template from the logic for better readability. Added type hints for all parameters and return type. Implemented error handling when writing to the file to manage potential I/O issues gracefully.</p>\n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"step\">\n",
      "        <h2>Step 6: Refactor the `generate_response` Method</h2>\n",
      "        <div class=\"explanation\">\n",
      "            <p>Enhancing the `generate_response` method by adding type hints, handling potential exceptions during the API call, and improving the message formatting ensures better reliability and clarity. Additionally, separating message construction from the API call improves modularity.</p>\n",
      "        </div>\n",
      "        <div class=\"code-container\">\n",
      "            <div>\n",
      "                <h3>Original Code</h3>\n",
      "                <pre><code class=\"python\">def generate_response(self, prompt, system_prompt, file_name=\"o1_response_test.html\", display_output=False):\n",
      "    \"\"\"\n",
      "    Public method to generate a response using the O1 model.\n",
      "    \"\"\"\n",
      "    start_time = time.time()\n",
      "\n",
      "    response = self.client.chat.completions.create(\n",
      "        model=self.o1_model,\n",
      "        messages=[\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": f\"<System Prompt>{system_prompt}</System Prompt>\\n<USER Prompt>{prompt}</USER Prompt>\"\n",
      "            }\n",
      "        ]\n",
      "    )\n",
      "\n",
      "    response_time = time.time() - start_time\n",
      "    usage = response.usage\n",
      "\n",
      "    total_cost = self._calculate_cost(\n",
      "        usage.prompt_tokens,\n",
      "        usage.prompt_tokens_details.cached_tokens,\n",
      "        usage.completion_tokens\n",
      "    )\n",
      "\n",
      "    response_content = response.choices[0].message.content\n",
      "\n",
      "    if display_output:\n",
      "        print(f\"Model: {self.o1_model}\")\n",
      "        print(f\"Response time: {response_time:.2f} seconds\")\n",
      "        print(f\"Total Cost: {total_cost:.2f} $\\n\")\n",
      "        print(response_content)\n",
      "\n",
      "    # Save to HTML\n",
      "    self._save_response_html(file_name, response_time, total_cost, response_content)\n",
      "\n",
      "    return response</code></pre>\n",
      "            </div>\n",
      "            <div>\n",
      "                <h3>Refactored Code</h3>\n",
      "                <pre><code class=\"python\">def generate_response(\n",
      "    self, \n",
      "    prompt: str, \n",
      "    system_prompt: str, \n",
      "    file_name: str = \"o1_response_test.html\", \n",
      "    display_output: bool = False\n",
      ") -> OpenAI.Response:\n",
      "    \"\"\"\n",
      "    Public method to generate a response using the O1 model.\n",
      "    \"\"\"\n",
      "    start_time = time.time()\n",
      "    try:\n",
      "        messages = [\n",
      "            {\n",
      "                \"role\": \"system\",\n",
      "                \"content\": system_prompt\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": prompt\n",
      "            }\n",
      "        ]\n",
      "        response = self.client.chat.completions.create(\n",
      "            model=self.o1_model,\n",
      "            messages=messages\n",
      "        )\n",
      "    except Exception as e:\n",
      "        raise RuntimeError(f\"Failed to generate response: {e}\") from e\n",
      "\n",
      "    response_time = time.time() - start_time\n",
      "    usage = response.usage\n",
      "\n",
      "    total_cost = self._calculate_cost(\n",
      "        prompt_tokens=usage.prompt_tokens,\n",
      "        cached_input_tokens=usage.prompt_tokens_details.cached_tokens,\n",
      "        output_tokens=usage.completion_tokens\n",
      "    )\n",
      "\n",
      "    response_content = response.choices[0].message.content\n",
      "\n",
      "    if display_output:\n",
      "        print(f\"Model: {self.o1_model}\")\n",
      "        print(f\"Response time: {response_time:.2f} seconds\")\n",
      "        print(f\"Total Cost: {total_cost:.2f} $\\n\")\n",
      "        print(response_content)\n",
      "\n",
      "    # Save to HTML\n",
      "    self._save_response_html(file_name, response_time, total_cost, response_content)\n",
      "\n",
      "    return response</code></pre>\n",
      "            </div>\n",
      "        </div>\n",
      "        <div class=\"improvements\">\n",
      "            <p><strong>Improvements:</strong> Added type hints for all parameters and return type. Changed the message roles to follow OpenAI's standard roles (`system` and `user`) for better compatibility. Introduced a try-except block to handle exceptions during the API call, providing clearer error messages. This refactoring enhances robustness and aligns the message structure with best practices.</p>\n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"step\">\n",
      "        <h2>Step 7: Finalizing the Object-Oriented Design</h2>\n",
      "        <div class=\"explanation\">\n",
      "            <p>Encapsulating the entire functionality within the `O1ResponseGenerator` class following best OOP principles ensures a clean and maintainable codebase. This final step ensures that all components interact seamlessly within the object-oriented framework.</p>\n",
      "        </div>\n",
      "        <div class=\"code-container\">\n",
      "            <div>\n",
      "                <h3>Original Code</h3>\n",
      "                <pre><code class=\"python\"># Entire original class as provided</code></pre>\n",
      "            </div>\n",
      "            <div>\n",
      "                <h3>Refactored Code</h3>\n",
      "                <pre><code class=\"python\">import time\n",
      "import markdown\n",
      "from dataclasses import dataclass\n",
      "from openai import OpenAI\n",
      "from helper import get_openai_api_key\n",
      "\n",
      "@dataclass(frozen=True)\n",
      "class ModelPrices:\n",
      "    input_token_price: float\n",
      "    cached_input_token_price: float\n",
      "    output_token_price: float\n",
      "\n",
      "MODEL_PRICES = {\n",
      "    \"o1\": ModelPrices(\n",
      "        input_token_price=15.00 / 1_000_000,\n",
      "        cached_input_token_price=7.50 / 1_000_000,\n",
      "        output_token_price=60.00 / 1_000_000,\n",
      "    ),\n",
      "    \"o1-mini\": ModelPrices(\n",
      "        input_token_price=3.00 / 1_000_000,\n",
      "        cached_input_token_price=1.50 / 1_000_000,\n",
      "        output_token_price=12.00 / 1_000_000,\n",
      "    ),\n",
      "}\n",
      "\n",
      "class O1ResponseGenerator:\n",
      "    def __init__(self, o1_model: str = \"o1\"):\n",
      "        \"\"\"\n",
      "        Initialize the O1ResponseGenerator with a model name.\n",
      "        \"\"\"\n",
      "        self.o1_model = o1_model\n",
      "        try:\n",
      "            openai_api_key = get_openai_api_key()\n",
      "            if not openai_api_key:\n",
      "                raise ValueError(\"OpenAI API key is missing.\")\n",
      "            self.client = OpenAI(api_key=openai_api_key)\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Failed to initialize OpenAI client: {e}\") from e\n",
      "\n",
      "    def _calculate_cost(self, prompt_tokens: int, cached_input_tokens: int, output_tokens: int) -> float:\n",
      "        \"\"\"\n",
      "        Internal method to calculate the total cost based on tokens used.\n",
      "        \"\"\"\n",
      "        prices = MODEL_PRICES.get(self.o1_model, MODEL_PRICES[\"o1\"])\n",
      "\n",
      "        input_token_cost = (prompt_tokens - cached_input_tokens) * prices.input_token_price\n",
      "        cached_input_token_cost = cached_input_tokens * prices.cached_input_token_price\n",
      "        output_token_cost = output_tokens * prices.output_token_price\n",
      "\n",
      "        total_cost = input_token_cost + cached_input_token_cost + output_token_cost\n",
      "        return total_cost\n",
      "\n",
      "    def _save_response_html(\n",
      "        self, \n",
      "        file_name: str, \n",
      "        response_time: float, \n",
      "        total_cost: float, \n",
      "        response_content: str\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Internal method to generate and save HTML from the response content.\n",
      "        \"\"\"\n",
      "        response_html = markdown.markdown(response_content)\n",
      "        html_template = \"\"\"\n",
      "        <html>\n",
      "        <head>\n",
      "            <meta charset=\"UTF-8\">\n",
      "            <title>O1 Response</title>\n",
      "            <script type=\"text/javascript\" async \n",
      "                src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
      "            </script>\n",
      "        </head>\n",
      "        <body>\n",
      "            <div style=\"background-color: #f0fff8; padding: 10px; border-radius: 5px; border: 1px solid #d3d3d3;\">\n",
      "                <h2>{model} Model Response</h2>\n",
      "                <p>Response time: {time:.2f} seconds</p>\n",
      "                <p>Total Cost: {cost:.2f} $</p>\n",
      "            </div>\n",
      "\n",
      "            <div style=\"padding: 10px;\">\n",
      "                {content}\n",
      "            </div>\n",
      "        </body>\n",
      "        </html>\n",
      "        \"\"\"\n",
      "        html_content = html_template.format(\n",
      "            model=self.o1_model,\n",
      "            time=response_time,\n",
      "            cost=total_cost,\n",
      "            content=response_html\n",
      "        )\n",
      "        try:\n",
      "            with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
      "                file.write(html_content)\n",
      "            print(f\"Response saved to {file_name}\")\n",
      "        except IOError as e:\n",
      "            print(f\"Failed to save response to {file_name}: {e}\")\n",
      "\n",
      "    def generate_response(\n",
      "        self, \n",
      "        prompt: str, \n",
      "        system_prompt: str, \n",
      "        file_name: str = \"o1_response_test.html\", \n",
      "        display_output: bool = False\n",
      "    ) -> OpenAI.Response:\n",
      "        \"\"\"\n",
      "        Public method to generate a response using the O1 model.\n",
      "        \"\"\"\n",
      "        start_time = time.time()\n",
      "        try:\n",
      "            messages = [\n",
      "                {\n",
      "                    \"role\": \"system\",\n",
      "                    \"content\": system_prompt\n",
      "                },\n",
      "                {\n",
      "                    \"role\": \"user\",\n",
      "                    \"content\": prompt\n",
      "                }\n",
      "            ]\n",
      "            response = self.client.chat.completions.create(\n",
      "                model=self.o1_model,\n",
      "                messages=messages\n",
      "            )\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Failed to generate response: {e}\") from e\n",
      "\n",
      "        response_time = time.time() - start_time\n",
      "        usage = response.usage\n",
      "\n",
      "        total_cost = self._calculate_cost(\n",
      "            prompt_tokens=usage.prompt_tokens,\n",
      "            cached_input_tokens=usage.prompt_tokens_details.cached_tokens,\n",
      "            output_tokens=usage.completion_tokens\n",
      "        )\n",
      "\n",
      "        response_content = response.choices[0].message.content\n",
      "\n",
      "        if display_output:\n",
      "            print(f\"Model: {self.o1_model}\")\n",
      "            print(f\"Response time: {response_time:.2f} seconds\")\n",
      "            print(f\"Total Cost: {total_cost:.2f} $\\n\")\n",
      "            print(response_content)\n",
      "\n",
      "        # Save to HTML\n",
      "        self._save_response_html(file_name, response_time, total_cost, response_content)\n",
      "\n",
      "        return response</code></pre>\n",
      "            </div>\n",
      "        </div>\n",
      "        <div class=\"improvements\">\n",
      "            <p><strong>Improvements:</strong> Consolidated all previous refactorings into the final class, ensuring consistency and adherence to object-oriented principles. The class now features clear separation of concerns, robust error handling, and enhanced readability, making it more maintainable and scalable for future developments.</p>\n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <h2>Conclusion</h2>\n",
      "    <p>The refactored `O1ResponseGenerator` class now benefits from optimized import statements, robust error handling, enhanced readability through the use of dataclasses, and improved method structures. These changes collectively make the code more maintainable, scalable, and less prone to bugs, adhering to best practices in Python programming and object-oriented design.</p>\n",
      "</body>\n",
      "</html>\n",
      "Response saved to code_update1.html\n"
     ]
    }
   ],
   "source": [
    "response_generator = O1ResponseGenerator(o1_model=\"o1-mini\")\n",
    "# Generate the response\n",
    "response = response_generator.generate_response(\n",
    "    prompt=prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    file_name=\"code_update1.html\",\n",
    "    display_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaba142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
