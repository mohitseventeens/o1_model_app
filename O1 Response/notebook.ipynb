{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "\n",
    "pip install openai, markdown, python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your utilities or helper functions to this file.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# these expect to find a .env file at the directory above the lesson.                                                                                                                     # the format for that file is (without the comment)                                                                                                                                       #API_KEYNAME=AStringThatIsTheLongAPIKeyFromSomeService                                                                                                                                     \n",
    "def load_env():\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "\n",
    "def get_openai_api_key():\n",
    "    load_env()\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    return openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from helper import get_openai_api_key\n",
    "from openai import OpenAI\n",
    "import markdown\n",
    "\n",
    "class O1ResponseGenerator:\n",
    "    def __init__(self, o1_model=\"o1-mini\"):\n",
    "        \"\"\"\n",
    "        Initialize the O1ResponseGenerator with a model name.\n",
    "        \"\"\"\n",
    "        self.o1_model = o1_model\n",
    "        openai_api_key = get_openai_api_key()\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    def _calculate_cost(self, prompt_tokens, cached_input_tokens, output_tokens):\n",
    "        \"\"\"\n",
    "        Internal method to calculate the total cost based on tokens used.\n",
    "        \"\"\"\n",
    "        model_prices = {\n",
    "            \"o1\": {\n",
    "                \"input_token_price\": 15.00 / 1_000_000,\n",
    "                \"cached_input_token_price\": 7.50 / 1_000_000,\n",
    "                \"output_token_price\": 60.00 / 1_000_000,\n",
    "            },\n",
    "            \"o1-mini\": {\n",
    "                \"input_token_price\": 3.00 / 1_000_000,\n",
    "                \"cached_input_token_price\": 1.50 / 1_000_000,\n",
    "                \"output_token_price\": 12.00 / 1_000_000,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        if self.o1_model not in model_prices:\n",
    "            self.o1_model = \"o1\"\n",
    "\n",
    "        prices = model_prices[self.o1_model]\n",
    "        input_token_cost = (prompt_tokens - cached_input_tokens) * prices[\"input_token_price\"]\n",
    "        cached_input_token_cost = cached_input_tokens * prices[\"cached_input_token_price\"]\n",
    "        output_token_cost = output_tokens * prices[\"output_token_price\"]\n",
    "\n",
    "        return input_token_cost + cached_input_token_cost + output_token_cost\n",
    "\n",
    "    def _save_response_html(self, file_name, response_time, total_cost, response_content):\n",
    "        \"\"\"\n",
    "        Internal method to generate and save HTML from the response content.\n",
    "        \"\"\"\n",
    "        response_html = markdown.markdown(response_content)\n",
    "        html_content = f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "            <meta charset=\"UTF-8\">\n",
    "            <title>O1 Response</title>\n",
    "            <script type=\"text/javascript\" async \n",
    "                src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "            </script>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div style=\"background-color: #f0fff8; padding: 10px; border-radius: 5px; border: 1px solid #d3d3d3;\">\n",
    "                <h2>{self.o1_model} Model Response</h2>\n",
    "                <p>Response time: {response_time:.2f} seconds</p>\n",
    "                <p>Total Cost: {total_cost:.2f} $</p>\n",
    "            </div>\n",
    "\n",
    "            <div style=\"padding: 10px;\">\n",
    "                {response_html}\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html_content)\n",
    "\n",
    "        print(f\"Response saved to {file_name}\")\n",
    "\n",
    "    def generate_response(self, prompt, system_prompt, file_name=\"o1_response_test.html\"):\n",
    "        \"\"\"\n",
    "        Public method to generate a response using the O1 model.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.o1_model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"<System Prompt>{system_prompt}</System Prompt>\\\\n<USER Prompt>{prompt}</USER Prompt>\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        response_time = time.time() - start_time\n",
    "        usage = response.usage\n",
    "\n",
    "        total_cost = self._calculate_cost(\n",
    "            usage.prompt_tokens,\n",
    "            usage.prompt_tokens_details.cached_tokens,\n",
    "            usage.completion_tokens\n",
    "        )\n",
    "\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "        # Save to HTML\n",
    "        self._save_response_html(file_name, response_time, total_cost, response_content)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from o1_response_generator import O1ResponseGenerator\n",
    "from prompts import system_prompt, prompt\n",
    "\n",
    "# I've imported system_prompt and prompt from prompts.py\n",
    "# prompts.py contains dictionaries for system_prompt and prompt (NOT necessary)\n",
    "# Pass your system_prompt and prompt to O1ResponseGenerator\n",
    "# Edit the file_name to save \n",
    "\n",
    "# system_prompt = '''\n",
    "# [Enter system Prompt here]\n",
    "# '''\n",
    "# prompt = '''\n",
    "# [Enter user Prompt here]\n",
    "# '''\n",
    "\n",
    "# Note: The output is saved in HTML and best output is generated by 'o1'\n",
    "# if 'o1-preview' and 'o1-mini' are used, the output is good but doent look the best\n",
    "\n",
    "response_generator = O1ResponseGenerator(o1_model=\"o1-preview\")\n",
    "# Generate the response\n",
    "response = response_generator.generate_response(\n",
    "    prompt=prompt,\n",
    "    system_prompt=system_prompt,\n",
    "    file_name=\"response_output.html\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
